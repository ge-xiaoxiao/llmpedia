import{_ as r}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as i,b as e,d as o,e as n,r as s,o as m}from"./app-CRXmQLa0.js";const l={},c={id:"mqa-gqa",tabindex:"-1"},d={class:"header-anchor",href:"#mqa-gqa"},p={id:"mla",tabindex:"-1"},h={class:"header-anchor",href:"#mla"};function g(u,t){const a=s("Badge");return m(),i("div",null,[t[2]||(t[2]=e("h1",{id:"注意力机制及其变种",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#注意力机制及其变种"},[e("span",null,"注意力机制及其变种")])],-1)),e("h2",c,[e("a",d,[e("span",null,[t[0]||(t[0]=o("MQA & GQA ",-1)),n(a,{text:"重要",type:"danger"})])])]),e("h2",p,[e("a",h,[e("span",null,[t[1]||(t[1]=o("MLA ",-1)),n(a,{text:"重要",type:"danger"})])])])])}const f=r(l,[["render",g]]),y=JSON.parse('{"path":"/docs/llm-architecture/attention-mechanisms.html","title":"注意力机制及其变种","lang":"zh-CN","frontmatter":{"description":"注意力机制及其变种 MQA & GQA MLA","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"注意力机制及其变种\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2026-01-30T14:31:16.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"LLMPedia\\",\\"url\\":\\"https://llmpedia.online\\"}]}"],["meta",{"property":"og:url","content":"https://llmpedia.online/docs/llm-architecture/attention-mechanisms.html"}],["meta",{"property":"og:site_name","content":"LLMPedia"}],["meta",{"property":"og:title","content":"注意力机制及其变种"}],["meta",{"property":"og:description","content":"注意力机制及其变种 MQA & GQA MLA"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2026-01-30T14:31:16.000Z"}],["meta",{"property":"article:modified_time","content":"2026-01-30T14:31:16.000Z"}]]},"git":{"createdTime":1769771827000,"updatedTime":1769783476000,"contributors":[{"name":"ge-xiaoxiao","username":"ge-xiaoxiao","email":"marshall_gefxiang@163.com","commits":1,"url":"https://github.com/ge-xiaoxiao"},{"name":"Marshall-Ge","username":"Marshall-Ge","email":"1004083966@qq.com","commits":1,"url":"https://github.com/Marshall-Ge"}]},"readingTime":{"minutes":0.09,"words":26},"filePathRelative":"docs/llm-architecture/attention-mechanisms.md","autoDesc":true}');export{f as comp,y as data};
