import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,a as t,o as n}from"./app-DnqpEzx0.js";const h={};function e(l,s){return n(),a("div",null,[...s[0]||(s[0]=[t(`<h1 id="llm架构面试题" tabindex="-1"><a class="header-anchor" href="#llm架构面试题"><span>LLM架构面试题</span></a></h1><p>本章节涵盖大语言模型架构相关的面试题，包括Transformer、注意力机制、预训练与微调等核心知识点。</p><h2 id="核心知识点" tabindex="-1"><a class="header-anchor" href="#核心知识点"><span>核心知识点</span></a></h2><h3 id="_1-transformer架构" tabindex="-1"><a class="header-anchor" href="#_1-transformer架构"><span>1. Transformer架构</span></a></h3><ul><li><strong>自注意力机制</strong>：原理、计算复杂度、多头注意力</li><li><strong>位置编码</strong>：绝对位置编码、相对位置编码、RoPE</li><li><strong>前馈神经网络</strong>：FFN结构、激活函数选择</li><li><strong>层归一化</strong>：Pre-LN vs Post-LN</li></ul><h3 id="_2-预训练与微调" tabindex="-1"><a class="header-anchor" href="#_2-预训练与微调"><span>2. 预训练与微调</span></a></h3><ul><li><strong>预训练目标</strong>：MLM、NSP、Span Masking</li><li><strong>微调策略</strong>：全参数微调、LoRA、P-Tuning</li><li><strong>指令微调</strong>：SFT、RLHF、DPO</li><li><strong>领域适配</strong>：持续预训练、领域自适应</li></ul><h3 id="_3-模型优化" tabindex="-1"><a class="header-anchor" href="#_3-模型优化"><span>3. 模型优化</span></a></h3><ul><li><strong>模型压缩</strong>：知识蒸馏、量化、剪枝</li><li><strong>推理优化</strong>：KV Cache、Flash Attention、Paged Attention</li><li><strong>训练优化</strong>：混合精度训练、梯度检查点、ZeRO优化器</li></ul><h2 id="常见面试题" tabindex="-1"><a class="header-anchor" href="#常见面试题"><span>常见面试题</span></a></h2><h3 id="基础题" tabindex="-1"><a class="header-anchor" href="#基础题"><span>基础题</span></a></h3><ol><li><p><strong>Q：请解释Transformer中的自注意力机制</strong><br> A：自注意力机制允许模型在处理序列时关注输入序列的不同位置...</p></li><li><p><strong>Q：多头注意力有什么优势？</strong><br> A：多头注意力可以让模型同时关注来自不同表示子空间的信息...</p></li></ol><h3 id="进阶题" tabindex="-1"><a class="header-anchor" href="#进阶题"><span>进阶题</span></a></h3><ol><li><p><strong>Q：对比不同位置编码方法的优缺点</strong><br> A：绝对位置编码（如Sinusoidal）简单但泛化能力有限...</p></li><li><p><strong>Q：如何优化大模型的推理速度？</strong><br> A：可以使用KV Cache减少重复计算，Flash Attention优化注意力计算...</p></li></ol><h2 id="实战问题" tabindex="-1"><a class="header-anchor" href="#实战问题"><span>实战问题</span></a></h2><h3 id="系统设计" tabindex="-1"><a class="header-anchor" href="#系统设计"><span>系统设计</span></a></h3><ol><li><strong>设计一个支持千亿参数模型推理的系统架构</strong></li><li><strong>如何实现高效的模型并行训练</strong></li></ol><h3 id="代码实现" tabindex="-1"><a class="header-anchor" href="#代码实现"><span>代码实现</span></a></h3><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 简化的注意力机制实现</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> attention</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">query</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;"> key</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;"> value</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;"> mask</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">None</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    d_k </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> query.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">size</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    scores </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">matmul</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(query, key.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">transpose</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">2</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)) </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> math.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">sqrt</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(d_k)</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    if</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> mask </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">is</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> not</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> None</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        scores </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> scores.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">masked_fill</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(mask </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">==</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1e9</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    p_attn </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> F.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">softmax</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(scores, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">dim</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=-</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    return</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">matmul</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(p_attn, value), p_attn</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,19)])])}const p=i(h,[["render",e]]),d=JSON.parse('{"path":"/docs/llm-architecture/","title":"LLM架构面试题","lang":"zh-CN","frontmatter":{"description":"LLM架构面试题 本章节涵盖大语言模型架构相关的面试题，包括Transformer、注意力机制、预训练与微调等核心知识点。 核心知识点 1. Transformer架构 自注意力机制：原理、计算复杂度、多头注意力 位置编码：绝对位置编码、相对位置编码、RoPE 前馈神经网络：FFN结构、激活函数选择 层归一化：Pre-LN vs Post-LN 2. ...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"LLM架构面试题\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2026-01-29T10:42:58.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"LLMPedia\\",\\"url\\":\\"https://llmpedia.example.com\\"}]}"],["meta",{"property":"og:url","content":"https://llmpedia.example.com/docs/llm-architecture/"}],["meta",{"property":"og:site_name","content":"LLMPedia - 大模型面试八股题"}],["meta",{"property":"og:title","content":"LLM架构面试题"}],["meta",{"property":"og:description","content":"LLM架构面试题 本章节涵盖大语言模型架构相关的面试题，包括Transformer、注意力机制、预训练与微调等核心知识点。 核心知识点 1. Transformer架构 自注意力机制：原理、计算复杂度、多头注意力 位置编码：绝对位置编码、相对位置编码、RoPE 前馈神经网络：FFN结构、激活函数选择 层归一化：Pre-LN vs Post-LN 2. ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2026-01-29T10:42:58.000Z"}],["meta",{"property":"article:modified_time","content":"2026-01-29T10:42:58.000Z"}]]},"git":{"createdTime":1769683378000,"updatedTime":1769683378000,"contributors":[{"name":"ge-xiaoxiao","username":"ge-xiaoxiao","email":"marshall_gefxiang@163.com","commits":1,"url":"https://github.com/ge-xiaoxiao"}]},"readingTime":{"minutes":1.56,"words":469},"filePathRelative":"docs/llm-architecture/README.md","autoDesc":true}');export{p as comp,d as data};
