import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as r,b as e,d as n,e as l,a as p,r as a,o as s}from"./app-CRXmQLa0.js";const d={},m={id:"基础概念",tabindex:"-1"},u={class:"header-anchor",href:"#基础概念"},P={id:"强化学习方法分类",tabindex:"-1"},g={class:"header-anchor",href:"#强化学习方法分类"},h={id:"ppo原理及其优缺点",tabindex:"-1"},O={class:"header-anchor",href:"#ppo原理及其优缺点"},f={id:"dpo的原理及其优缺点",tabindex:"-1"},y={class:"header-anchor",href:"#dpo的原理及其优缺点"},x={id:"grpo和ppo有什么区别",tabindex:"-1"},b={class:"header-anchor",href:"#grpo和ppo有什么区别"},R={id:"sft是必要的吗",tabindex:"-1"},T={class:"header-anchor",href:"#sft是必要的吗"};function D(S,t){const o=a("Badge");return s(),r("div",null,[t[6]||(t[6]=e("h1",{id:"强化学习",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#强化学习"},[e("span",null,"强化学习")])],-1)),e("h2",m,[e("a",u,[e("span",null,[t[0]||(t[0]=n("基础概念 ",-1)),l(o,{text:"重要",type:"danger"})])])]),t[7]||(t[7]=e("p",null,"强化学习（RL）是智能体通过与环境交互，以试错方式学习策略的机器学习范式，核心是通过奖励信号引导行为优化，实现从状态到动作的映射，目标是最大化长期累积奖励。核心要素包含：智能体（Agent）、环境（Environment）、状态（State）、动作（Action）、奖励（Reward）、策略（Policy）、价值函数（Value Function）、马尔可夫决策过程（MDP，RL 问题的数学建模框架）。",-1)),e("h2",P,[e("a",g,[e("span",null,[t[1]||(t[1]=n("强化学习方法分类 ",-1)),l(o,{text:"掌握",type:"tip"})])])]),t[8]||(t[8]=p("<p>按模型知识：</p><ul><li><p>模型基于（Model-based）：学习环境动态模型</p></li><li><p>模型无关（Model-free）：直接学习策略/价值函数</p></li></ul><p>按学习方式：</p><ul><li><p>价值基于（Value-based）：学习价值函数（如Q-learning、DQN）</p></li><li><p>策略基于（Policy-based）：直接学习策略函数（如REINFORCE）</p></li><li><p>演员-评论家（Actor-Critic）：结合两者（如A2C、PPO）</p></li></ul><p>按策略类型：</p><ul><li><p>同策略（On-policy）：使用当前策略收集数据（如PPO）</p></li><li><p>异策略（Off-policy）：使用不同策略收集数据（如DQN、DDPG）</p></li></ul><p>按数据来源分：</p><ul><li><p>在线强化学习：智能体与环境进行实时交互、实时获得反馈并学习更新策略。</p></li><li><p>离线强化学习：智能体与环境无实时交互，通过事先收集的离线数据集学习。</p></li></ul>",8)),e("h2",h,[e("a",O,[e("span",null,[t[2]||(t[2]=n("PPO原理及其优缺点 ",-1)),l(o,{text:"重要",type:"danger"})])])]),t[9]||(t[9]=e("p",null,[e("a",{href:"https://zhuanlan.zhihu.com/p/677607581",target:"_blank",rel:"noopener noreferrer"},"PPO原理请参考这个链接🔗")],-1)),t[10]||(t[10]=e("p",null,"PPO（近端策略优化）的核心思想是通过限制策略更新的幅度来确保训练稳定性。它采用裁剪机制约束新旧策略的差异，避免因单次更新过大而导致策略性能崩溃。具体实现中，PPO计算新旧策略的概率比，将其限制在[1-ε, 1+ε]范围内，然后基于优势函数优化目标。",-1)),t[11]||(t[11]=e("p",null,"调试时需要综合观察多个指标：回报曲线、KL散度、裁剪频率和价值损失。如果裁剪频率超过15%，应考虑增大ε；若低于5%可减小ε。价值损失应与策略损失保持同量级，如果过大则需要降低价值网络学习率。回报曲线的剧烈波动往往表明学习率过大或批次大小不足。",-1)),t[12]||(t[12]=e("p",null,"PPO的主要优点是实现相对简单、训练稳定、通用性强，适用于各类任务。缺点在于超参数仍需仔细调整、理论保证较弱、探索能力有限，且作为同策略算法在样本效率上不如异策略方法。",-1)),e("h2",f,[e("a",y,[e("span",null,[t[3]||(t[3]=n("DPO的原理及其优缺点 ",-1)),l(o,{text:"重要",type:"danger"})])])]),t[13]||(t[13]=e("p",null,[e("a",{href:"https://zhuanlan.zhihu.com/p/1984387073625593089",target:"_blank",rel:"noopener noreferrer"},"DPO、GRPO原理可以参考这个链接🔗"),e("br"),n(" DPO（直接偏好优化）是一种绕过显式奖励建模的强化学习方法。它基于Bradley-Terry模型，直接利用人类偏好数据优化策略，将强化学习问题转化为偏好分类问题。DPO的核心洞察是，对于最优策略，其偏好概率与奖励差异之间存在解析关系，因此可以直接从偏好数据中学习策略，而无需训练单独的奖励模型。")],-1)),t[14]||(t[14]=e("p",null,"DPO的优点主要体现在简化训练流程、提高计算效率和增强稳定性方面。它消除了奖励模型的训练环节，避免了奖励函数设计困难、奖励劫持和过度优化等问题。同时，DPO直接优化策略偏好，训练过程更加稳定可控。",-1)),t[15]||(t[15]=e("p",null,"然而DPO也存在明显局限。它对偏好数据的质量和数量要求很高，需要覆盖多样化的场景和比较对；难以融入额外的奖励信号或约束条件；在新领域的泛化能力可能受限；且缺乏显式奖励函数，不利于后续的分析和调试。尽管如此，在语言模型对齐等任务中，DPO因其简单高效而成为重要方法。",-1)),e("h2",x,[e("a",b,[e("span",null,[t[4]||(t[4]=n("GRPO和PPO有什么区别 ",-1)),l(o,{text:"重要",type:"danger"})])])]),t[16]||(t[16]=e("p",null,"GRPO（分组相对策略优化）在PPO基础上引入了分组比较机制。核心改进在于将样本分组，在组内计算相对奖励进行标准化，而非使用传统的优势函数估计。这种设计能更好地处理奖励稀疏和方差大的问题，提升训练稳定性。",-1)),t[17]||(t[17]=e("p",null,"具体区别体现在几个方面：奖励处理上，GRPO采用组内相对比较，减少了全局奖励方差；更新机制上，GRPO以组为单位进行策略优化，支持组内样本的重复利用；并行效率上，GRPO更适合大规模并行环境，在多智能体任务中表现出色。GRPO通过分组比较降低了估计方差，在奖励稀疏的任务中通常表现更稳定，但实现相对复杂，对分组策略敏感。",-1)),e("h2",R,[e("a",T,[e("span",null,[t[5]||(t[5]=n("SFT是必要的吗 ",-1)),l(o,{text:"掌握",type:"tip"})])])]),t[18]||(t[18]=e("blockquote",null,[e("p",null,"关键词：冷启动、涌现")],-1)),t[19]||(t[19]=e("p",null,"监督微调（SFT）的必要性需结合具体场景评估。在强化学习尤其是语言模型对齐中，SFT通常作为重要的预处理阶段，为后续的强化学习提供合理的初始策略。这对于冷启动至关重要，能够避免从完全随机策略开始探索的低效性，并确保基本的安全性和能力水平。SFT利用现有标注数据快速建立基础能力，为后续的强化学习提供必要的技能基础。",-1)),t[20]||(t[20]=e("p",null,"然而，SFT并非绝对必需。当环境允许低成本试错探索、奖励信号设计明确且丰富、或者任务需要完全自主的探索发现时，可以直接从强化学习开始。在计算资源受限的情况下，跳过SFT直接进行强化学习有时会更高效。(Deepseek-V3跳过了SFT，直接上RL, 出现了涌现现象)",-1)),t[21]||(t[21]=e("p",null,"实际应用中需要权衡多方面因素：任务的安全要求、可用标注数据的质量和数量、计算资源限制以及最终性能目标等。在大型语言模型的训练中，SFT通常被视为关键环节，它为后续的偏好对齐提供了必要的初始能力和安全性基础，但具体实施时应根据任务特性和资源约束做出合理决策。",-1))])}const A=i(d,[["render",D]]),L=JSON.parse('{"path":"/docs/algorithm-optimization/rl-alignment.html","title":"强化学习","lang":"zh-CN","frontmatter":{"description":"强化学习 基础概念 强化学习（RL）是智能体通过与环境交互，以试错方式学习策略的机器学习范式，核心是通过奖励信号引导行为优化，实现从状态到动作的映射，目标是最大化长期累积奖励。核心要素包含：智能体（Agent）、环境（Environment）、状态（State）、动作（Action）、奖励（Reward）、策略（Policy）、价值函数（Value F...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"强化学习\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2026-01-30T14:31:16.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"LLMPedia\\",\\"url\\":\\"https://llmpedia.online\\"}]}"],["meta",{"property":"og:url","content":"https://llmpedia.online/docs/algorithm-optimization/rl-alignment.html"}],["meta",{"property":"og:site_name","content":"LLMPedia"}],["meta",{"property":"og:title","content":"强化学习"}],["meta",{"property":"og:description","content":"强化学习 基础概念 强化学习（RL）是智能体通过与环境交互，以试错方式学习策略的机器学习范式，核心是通过奖励信号引导行为优化，实现从状态到动作的映射，目标是最大化长期累积奖励。核心要素包含：智能体（Agent）、环境（Environment）、状态（State）、动作（Action）、奖励（Reward）、策略（Policy）、价值函数（Value F..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2026-01-30T14:31:16.000Z"}],["meta",{"property":"article:modified_time","content":"2026-01-30T14:31:16.000Z"}]]},"git":{"createdTime":1769771827000,"updatedTime":1769783476000,"contributors":[{"name":"ge-xiaoxiao","username":"ge-xiaoxiao","email":"marshall_gefxiang@163.com","commits":1,"url":"https://github.com/ge-xiaoxiao"},{"name":"Marshall-Ge","username":"Marshall-Ge","email":"1004083966@qq.com","commits":1,"url":"https://github.com/Marshall-Ge"}]},"readingTime":{"minutes":5.44,"words":1632},"filePathRelative":"docs/algorithm-optimization/rl-alignment.md","autoDesc":true}');export{A as comp,L as data};
