import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as r,b as e,d as a,e as o,r as s,o as l}from"./app-CRXmQLa0.js";const d={},m={id:"vllm的pagedattention机制",tabindex:"-1"},p={class:"header-anchor",href:"#vllm的pagedattention机制"},c={id:"sglang的radixattention",tabindex:"-1"},g={class:"header-anchor",href:"#sglang的radixattention"},h={id:"sglang的流式输出、结构化生成等特性",tabindex:"-1"},f={class:"header-anchor",href:"#sglang的流式输出、结构化生成等特性"};function u(x,t){const n=s("Badge");return l(),r("div",null,[t[3]||(t[3]=e("h1",{id:"推理框架",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#推理框架"},[e("span",null,"推理框架")])],-1)),e("h2",m,[e("a",p,[e("span",null,[t[0]||(t[0]=a("vLLM的PagedAttention机制 ",-1)),o(n,{text:"重要",type:"danger"})])])]),t[4]||(t[4]=e("p",null,"vLLM的PagedAttention机制借鉴了操作系统的虚拟内存与分页思想。其核心是将所有序列的键值缓存（KV Cache）在物理内存中划分为固定大小的块，每个序列的KV Cache以非连续但逻辑映射的方式存储在这些块中。这一设计主要解决了传统动态内存分配导致的内存碎片化问题，从而将GPU内存利用率从通常不足50%提升至接近理论极限。更高的内存利用率允许在单次推理中批处理更多的请求，显著提升了吞吐量。此外，该机制天然支持不同序列间共享KV Cache块（例如在并行采样时共享提示前缀），实现了进一步的内存节约。PagedAttention是推动大模型服务从“能跑起来”到“能高效高并发运行”的关键底层创新。",-1)),e("h2",c,[e("a",g,[e("span",null,[t[1]||(t[1]=a("SGlang的RadixAttention ",-1)),o(n,{text:"重要",type:"danger"})])])]),t[5]||(t[5]=e("p",null,"SGlang的RadixAttention则针对另一种瓶颈：重复计算的公共前缀。它构建了一个全局的、基于前缀树（Trie）的KV Cache缓存系统。系统会自动识别并缓存来自不同请求的相同提示词前缀的计算结果。当新请求到达时，会通过前缀树快速匹配并复用最长公共前缀的KV Cache，仅需计算新增部分。该机制尤其优化了提示词存在大量重叠的场景（例如共享系统指令的聊天服务、基于同一份文档的多个RAG查询），能近乎彻底地消除重复计算。其最直接的性能收益体现在首Token延迟的显著降低和整体计算效率的提升，使交互式应用体验更为流畅。",-1)),e("h2",h,[e("a",f,[e("span",null,[t[2]||(t[2]=a("SGlang的流式输出、结构化生成等特性 ",-1)),o(n,{text:"掌握",type:"tip"})])])]),t[6]||(t[6]=e("p",null,"除RadixAttention外，SGlang在编程接口层面提供了旨在提升开发效率与用户体验的特性。其流式输出不仅支持Token的逐个生成与返回，更通过与RadixAttention的深度集成，使得即使在包含复杂提示逻辑的链式调用中，也能实现极低延迟的流式响应。在结构化生成方面，SGlang通过引入专属的原语或语法，允许开发者以声明式的方式定义输出格式（如JSON、列表或特定函数调用），模型将严格遵循该格式生成内容。这消除了传统应用中复杂的后处理与解析代码，提高了生成结果的可靠性与程序的可维护性。这些特性共同使SGlang成为一个兼顾后端推理效率与前端开发体验的高层框架。",-1))])}const P=i(d,[["render",u]]),_=JSON.parse('{"path":"/docs/ai-system-engineering/inference-optimization/inference-frameworks.html","title":"推理框架","lang":"zh-CN","frontmatter":{"description":"推理框架 vLLM的PagedAttention机制 vLLM的PagedAttention机制借鉴了操作系统的虚拟内存与分页思想。其核心是将所有序列的键值缓存（KV Cache）在物理内存中划分为固定大小的块，每个序列的KV Cache以非连续但逻辑映射的方式存储在这些块中。这一设计主要解决了传统动态内存分配导致的内存碎片化问题，从而将GPU内存利用...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"推理框架\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2026-01-30T14:31:16.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"LLMPedia\\",\\"url\\":\\"https://llmpedia.online\\"}]}"],["meta",{"property":"og:url","content":"https://llmpedia.online/docs/ai-system-engineering/inference-optimization/inference-frameworks.html"}],["meta",{"property":"og:site_name","content":"LLMPedia"}],["meta",{"property":"og:title","content":"推理框架"}],["meta",{"property":"og:description","content":"推理框架 vLLM的PagedAttention机制 vLLM的PagedAttention机制借鉴了操作系统的虚拟内存与分页思想。其核心是将所有序列的键值缓存（KV Cache）在物理内存中划分为固定大小的块，每个序列的KV Cache以非连续但逻辑映射的方式存储在这些块中。这一设计主要解决了传统动态内存分配导致的内存碎片化问题，从而将GPU内存利用..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2026-01-30T14:31:16.000Z"}],["meta",{"property":"article:modified_time","content":"2026-01-30T14:31:16.000Z"}]]},"git":{"createdTime":1769771827000,"updatedTime":1769783476000,"contributors":[{"name":"ge-xiaoxiao","username":"ge-xiaoxiao","email":"marshall_gefxiang@163.com","commits":1,"url":"https://github.com/ge-xiaoxiao"},{"name":"Marshall-Ge","username":"Marshall-Ge","email":"1004083966@qq.com","commits":1,"url":"https://github.com/Marshall-Ge"}]},"readingTime":{"minutes":2.39,"words":717},"filePathRelative":"docs/ai-system-engineering/inference-optimization/inference-frameworks.md","autoDesc":true}');export{P as comp,_ as data};
