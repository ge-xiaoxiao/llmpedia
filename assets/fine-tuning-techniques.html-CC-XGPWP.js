import{_ as l}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as i,b as o,d as n,e,a as s,r as a,o as g}from"./app-CRXmQLa0.js";const p={},d={id:"请解释全参数微调与参数高效微调的核心区别与应用场景。",tabindex:"-1"},m={class:"header-anchor",href:"#请解释全参数微调与参数高效微调的核心区别与应用场景。"},u={id:"lora的原理是什么-它为什么有效且受欢迎",tabindex:"-1"},h={class:"header-anchor",href:"#lora的原理是什么-它为什么有效且受欢迎"},x={id:"除了lora-你还了解哪些参数高效微调技术-简述其特点。",tabindex:"-1"},f={class:"header-anchor",href:"#除了lora-你还了解哪些参数高效微调技术-简述其特点。"},A={id:"指令微调与继续预训练有什么区别",tabindex:"-1"},y={class:"header-anchor",href:"#指令微调与继续预训练有什么区别"},L={id:"在微调中-如何防止灾难性遗忘-有哪些策略",tabindex:"-1"},b={class:"header-anchor",href:"#在微调中-如何防止灾难性遗忘-有哪些策略"},R={id:"进行指令微调时-数据构造有哪些关键注意事项",tabindex:"-1"},B={class:"header-anchor",href:"#进行指令微调时-数据构造有哪些关键注意事项"};function T(N,t){const r=a("Badge");return g(),i("div",null,[t[12]||(t[12]=o("h1",{id:"微调技术",tabindex:"-1"},[o("a",{class:"header-anchor",href:"#微调技术"},[o("span",null,"微调技术")])],-1)),o("h2",d,[o("a",m,[o("span",null,[t[0]||(t[0]=o("strong",null,"请解释全参数微调与参数高效微调的核心区别与应用场景。",-1)),t[1]||(t[1]=n()),e(r,{text:"掌握",type:"tip"})])])]),t[13]||(t[13]=o("p",null,[o("strong",null,"核心区别"),n("在于对模型参数的更新范围与方式。"),o("strong",null,"全参数微调"),n("会更新预训练模型的所有参数，相当于在新的数据集上对整个模型进行一次“再训练”。它能最大程度地适应下游任务，但计算成本、存储成本和过拟合风险都很高。"),o("strong",null,"参数高效微调"),n("则通过引入少量可训练的新参数（如LoRA中的低秩矩阵、Adapter中的小型神经网络模块）或冻结绝大部分原始参数、仅选择性更新部分参数（如BitFit仅更新偏置项），来达到接近全参数微调的性能。")],-1)),t[14]||(t[14]=o("p",null,[o("strong",null,"应用场景"),n("：全参数微调适用于数据充足、计算资源丰富且任务分布与预训练数据差异较大的场景，如领域专业化（医学、法律）。参数高效微调则是资源受限、需要快速迭代或部署多任务服务的首选，例如在消费级GPU上针对特定指令进行微调，或在单一云端服务中为不同客户维护多个轻量级微调模型。")],-1)),o("h2",u,[o("a",h,[o("span",null,[t[2]||(t[2]=o("strong",null,"LoRA的原理是什么？它为什么有效且受欢迎？",-1)),t[3]||(t[3]=n()),e(r,{text:"重要",type:"danger"})])])]),t[15]||(t[15]=s("<p><strong>原理</strong>：LoRA（低秩适应）假设模型在下游任务适应过程中的参数更新具有低秩特性。它不直接更新预训练权重矩阵 (W)，而是冻结 (W)，并注入一个旁路矩阵 ( \\Delta W = BA )，其中 (B) 和 (A) 是可训练的低秩矩阵（秩 (r \\ll \\min(d, k))）。前向传播变为 (h = Wx + BAx)。</p><p><strong>有效性及受欢迎的原因</strong>：</p><ol><li><strong>显著减少可训练参数</strong>：参数量从 (d \\times k) 降至 ( (d + k) \\times r )，通常减少数千至数万倍，极大降低了内存和存储开销。</li><li><strong>保持推理零延迟</strong>：微调完成后，可将 (BA) 合并回 (W)，得到一个与原始架构完全一致的模型，无需在推理时引入任何额外计算或延迟。</li><li><strong>模块化与可组合性</strong>：不同的LoRA模块可以像插件一样方便地切换、组合或叠加，便于实现多任务学习或风格融合。</li><li><strong>实践简便性</strong>：通常只需指定目标层（如Q、V投影矩阵）和秩 (r)，调参简单，收敛稳定，效果常能媲美全微调。</li></ol><hr>",4)),o("h2",x,[o("a",f,[o("span",null,[t[4]||(t[4]=o("strong",null,"除了LoRA，你还了解哪些参数高效微调技术？简述其特点。",-1)),t[5]||(t[5]=n()),e(r,{text:"掌握",type:"tip"})])])]),t[16]||(t[16]=o("ul",null,[o("li",null,[o("strong",null,"Adapter"),n("：在Transformer层的注意力或前馈网络后插入一个小型的、带瓶颈结构的前馈网络。特点是结构规整，但会改变模型架构，可能引入推理延迟，且通常效果略逊于LoRA。")]),o("li",null,[o("strong",null,"Prefix-tuning / Prompt-tuning"),n("：在输入层或模型深处添加可学习的虚拟Token（前缀）。完全冻结模型，仅优化这些前缀向量。非常节省参数，但对超参数（长度、初始化）敏感，且长序列会挤占有效上下文窗口。")]),o("li",null,[o("strong",null,"QLoRA"),n("：LoRA的量化升级版。在微调时将预训练模型量化为4-bit，并利用参数高效的LoRA适配器进行训练，同时使用一种新颖的高精度技术来减少量化误差。它使得在单张消费级GPU（如24GB）上微调超大模型（如65B）成为可能，是资源极度受限场景下的突破性技术。")])],-1)),o("h2",A,[o("a",y,[o("span",null,[t[6]||(t[6]=o("strong",null,"指令微调与继续预训练有什么区别？",-1)),t[7]||(t[7]=n()),e(r,{text:"掌握",type:"tip"})])])]),t[17]||(t[17]=s("<p><strong>目标不同</strong>：<strong>指令微调</strong>的目标是教会模型<strong>理解和服从人类指令</strong>。它使用（指令， 期望输出）的配对数据进行有监督训练，旨在激发出模型在预训练阶段已具备但未显式展现的指令跟随和对话能力，对齐人类的交互偏好。<strong>继续预训练</strong>的目标则是让模型<strong>吸收某个特定领域或风格的新知识</strong>。它在无标注或弱标注的领域文本（如医学文献、代码）上，沿用预训练的语言建模目标进行训练，旨在扩展或深化模型的知识库。</p><p><strong>数据与任务形式</strong>：指令微调数据是高质量的对话或任务完成样本；继续预训练数据是纯文本或代码片段。因此，指令微调是“对齐”过程，而继续预训练是“知识注入”过程。</p>",2)),o("h2",L,[o("a",b,[o("span",null,[t[8]||(t[8]=o("strong",null,"在微调中，如何防止灾难性遗忘？有哪些策略？",-1)),t[9]||(t[9]=n()),e(r,{text:"重要",type:"danger"})])])]),t[18]||(t[18]=s("<p>灾难性遗忘指模型在学习新任务/数据时，过度覆盖并丢失了原有通用知识与能力。<br><strong>核心策略包括</strong>：</p><ol><li><strong>数据混合</strong>：在新任务数据中混入少量高质量的原始预训练数据或通用任务数据，在训练中同时兼顾新旧分布。</li><li><strong>正则化技术</strong>： <ul><li><strong>弹性权重整合</strong>：对重要的旧任务参数施加更强的惩罚，限制其变化幅度。重要性可通过Fisher信息矩阵等估算。</li><li><strong>梯度方向约束</strong>：确保新任务的梯度更新方向与旧任务的重要梯度方向不至于完全冲突。</li></ul></li><li><strong>参数高效微调</strong>：如使用LoRA、Adapter等，通过冻结绝大部分原始参数，从根本上限制了模型“遗忘”的能力。</li><li><strong>多任务学习</strong>：如果有多个相关任务，从一开始就进行联合训练，使模型学习到更稳健的共享表示。</li></ol>",2)),o("h2",R,[o("a",B,[o("span",null,[t[10]||(t[10]=o("strong",null,"进行指令微调时，数据构造有哪些关键注意事项？",-1)),t[11]||(t[11]=n()),e(r,{text:"重要",type:"danger"})])])]),t[19]||(t[19]=s("<ol><li><strong>多样性</strong>：指令应覆盖格式、复杂度、领域的广泛范围（如开放式生成、摘要、问答、推理等）。</li><li><strong>质量与真实性</strong>：输出应由专家编写或严格验证，确保正确、有用、无害。避免引入错误或偏见。</li><li><strong>格式一致性</strong>：明确统一输出的格式要求（如JSON、列表、正式/非正式语气），并在数据中严格体现。</li><li><strong>负样本与拒绝能力</strong>：包含一些不合理、模糊或越界的指令，并配上模型礼貌拒绝或要求澄清的回应，以增强模型的安全边界。</li><li><strong>避免泄露答案格式</strong>：指令本身不应隐含答案的结构或内容，迫使模型真正理解任务。</li></ol>",1))])}const q=l(p,[["render",T]]),G=JSON.parse('{"path":"/docs/algorithm-optimization/fine-tuning-techniques.html","title":"微调技术","lang":"zh-CN","frontmatter":{"description":"微调技术 请解释全参数微调与参数高效微调的核心区别与应用场景。 核心区别在于对模型参数的更新范围与方式。全参数微调会更新预训练模型的所有参数，相当于在新的数据集上对整个模型进行一次“再训练”。它能最大程度地适应下游任务，但计算成本、存储成本和过拟合风险都很高。参数高效微调则通过引入少量可训练的新参数（如LoRA中的低秩矩阵、Adapter中的小型神经网...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"微调技术\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2026-01-30T14:31:16.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"LLMPedia\\",\\"url\\":\\"https://llmpedia.online\\"}]}"],["meta",{"property":"og:url","content":"https://llmpedia.online/docs/algorithm-optimization/fine-tuning-techniques.html"}],["meta",{"property":"og:site_name","content":"LLMPedia"}],["meta",{"property":"og:title","content":"微调技术"}],["meta",{"property":"og:description","content":"微调技术 请解释全参数微调与参数高效微调的核心区别与应用场景。 核心区别在于对模型参数的更新范围与方式。全参数微调会更新预训练模型的所有参数，相当于在新的数据集上对整个模型进行一次“再训练”。它能最大程度地适应下游任务，但计算成本、存储成本和过拟合风险都很高。参数高效微调则通过引入少量可训练的新参数（如LoRA中的低秩矩阵、Adapter中的小型神经网..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2026-01-30T14:31:16.000Z"}],["meta",{"property":"article:modified_time","content":"2026-01-30T14:31:16.000Z"}]]},"git":{"createdTime":1769771827000,"updatedTime":1769783476000,"contributors":[{"name":"ge-xiaoxiao","username":"ge-xiaoxiao","email":"marshall_gefxiang@163.com","commits":1,"url":"https://github.com/ge-xiaoxiao"},{"name":"Marshall-Ge","username":"Marshall-Ge","email":"1004083966@qq.com","commits":1,"url":"https://github.com/Marshall-Ge"}]},"readingTime":{"minutes":5.53,"words":1659},"filePathRelative":"docs/algorithm-optimization/fine-tuning-techniques.md","autoDesc":true}');export{q as comp,G as data};
