# 推理框架

## vLLM的PagedAttention机制 <Badge text="重要" type="danger" />

vLLM的PagedAttention机制借鉴了操作系统的虚拟内存与分页思想。其核心是将所有序列的键值缓存（KV Cache）在物理内存中划分为固定大小的块，每个序列的KV Cache以非连续但逻辑映射的方式存储在这些块中。这一设计主要解决了传统动态内存分配导致的内存碎片化问题，从而将GPU内存利用率从通常不足50%提升至接近理论极限。更高的内存利用率允许在单次推理中批处理更多的请求，显著提升了吞吐量。此外，该机制天然支持不同序列间共享KV Cache块（例如在并行采样时共享提示前缀），实现了进一步的内存节约。PagedAttention是推动大模型服务从“能跑起来”到“能高效高并发运行”的关键底层创新。

## SGlang的RadixAttention <Badge text="重要" type="danger" />

SGlang的RadixAttention则针对另一种瓶颈：重复计算的公共前缀。它构建了一个全局的、基于前缀树（Trie）的KV Cache缓存系统。系统会自动识别并缓存来自不同请求的相同提示词前缀的计算结果。当新请求到达时，会通过前缀树快速匹配并复用最长公共前缀的KV Cache，仅需计算新增部分。该机制尤其优化了提示词存在大量重叠的场景（例如共享系统指令的聊天服务、基于同一份文档的多个RAG查询），能近乎彻底地消除重复计算。其最直接的性能收益体现在首Token延迟的显著降低和整体计算效率的提升，使交互式应用体验更为流畅。

## SGlang的流式输出、结构化生成等特性 <Badge text="掌握" type="tip" />

除RadixAttention外，SGlang在编程接口层面提供了旨在提升开发效率与用户体验的特性。其流式输出不仅支持Token的逐个生成与返回，更通过与RadixAttention的深度集成，使得即使在包含复杂提示逻辑的链式调用中，也能实现极低延迟的流式响应。在结构化生成方面，SGlang通过引入专属的原语或语法，允许开发者以声明式的方式定义输出格式（如JSON、列表或特定函数调用），模型将严格遵循该格式生成内容。这消除了传统应用中复杂的后处理与解析代码，提高了生成结果的可靠性与程序的可维护性。这些特性共同使SGlang成为一个兼顾后端推理效率与前端开发体验的高层框架。

