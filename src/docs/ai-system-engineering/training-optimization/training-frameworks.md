# 训练框架

## FlashAttention v1 <Badge text="重要" type="danger" />

[推荐教程](https://www.bilibili.com/video/BV1fG9rYWEA8/?spm_id_from=333.337.search-card.all.click&vd_source=3a13163d3fccf0112a072a3a483b9046)

- 分块，Kernel Fusion，将多个独立的GPU操作（Kernel）合并为一个定制的Kernel

- Backward Recomputation，在反向传播过程中，重新计算前向传播的部分结果，而不是从HBM中读取，虽然这会增加一些计算量，但显著减少了HBM的内存占用，使得模型可以处理更长的序列，或者在有限的GPU内存下使用更大的批次大小

- Softmax Tiling：Attention分数矩阵分割成若干个小的块（Tile），然后逐个加载到SRAM中进行Softmax计算。在计算每个Tile的Softmax时，算法会利用之前计算的块的信息，以保证最终结果的正确性

[![](https://youke.xn--y7xa690gmna.cn/s1/2026/01/30/697cb5d25556c.webp)](https://youke.xn--y7xa690gmna.cn/s1/2026/01/30/697cb5d25556c.webp)

## FlashAttention v2 <Badge text="重要" type="danger" />

FlashAttention v2 是在 v1 基础上进一步优化的注意力计算算法，核心目标是通过更精细的硬件感知设计减少内存访问开销。其主要改进包括三个方面：
- 一是算法重排，通过调整计算顺序减少非矩阵乘法操作（如 softmax 归一化）的中间存储，将更多计算保持在 SRAM 高速缓存中；

- 二是更好的并行化策略，针对序列维度进行更有效的切分，充分利用 GPU 的线程块和线程束资源；

- 三是减少了 warps 之间的同步开销，通过改进的工作分配降低通信成本。这些优化使得 v2 相比 v1 实现了约 2 倍的训练速度提升，在长序列处理（如 8K 以上上下文）时优势尤为明显，同时保持了与标准注意力完全相同的数值结果。

## FlashAttention v3 <Badge text="掌握" type="tip" />

FlashAttention v3 在前代基础上进一步针对新硬件架构优化，特别是对NVIDIA Hopper架构的Tensor Core进行专门适配。主要创新包括对FP8等新数据类型的支持，通过动态稀疏性利用智能跳过注意力矩阵中的低权重区域，以及更灵活的切分策略以适应多样化的工作负载。

核心改进体现在三个层面：首先是计算模式优化，利用新一代GPU的异步复制和分布式共享内存特性，进一步隐藏内存访问延迟；其次是硬件原语利用，深度集成CUDA Graph和新型Tensor Core指令集；最后是算法适应性增强，原生支持更多注意力变体如滑动窗口、块稀疏等结构化模式。

实际性能上，v3在H100等新硬件上相比v2又有1.5-2倍提升，特别在大批量、长序列的训练场景中效果显著。但需注意v3硬件依赖性更强，在老一代GPU上可能无法充分发挥优势，且对某些特殊注意力模式的支持仍在完善中。

## Deepspeed ZeRO 1/2/3 <Badge text="重要" type="danger" />

DeepSpeed ZeRO是一套分级内存优化系统，通过数据并行框架内的智能状态划分消除冗余存储。其核心理念是将模型训练状态进行渐进式划分，从优化器状态到梯度再到参数，逐级增加划分粒度以换取更大的内存节省。

具体来看，ZeRO-1主要划分**优化器状态**，每个GPU只存储和更新完整优化器状态的一部分，节省约4倍内存；ZeRO-2增加**梯度划分**，反向传播后梯度被分区存储，仅在全规约时进行通信，内存节省扩大至约8倍；ZeRO-3则进一步划分**模型参数**，每个GPU仅缓存部分参数，通过动态获取和释放参数实现近乎线性的内存缩放，可训练比单个GPU内存大数十倍的模型。

选择策略时需要考虑通信开销与内存节省的权衡：ZeRO-1通信最少，适合网络带宽有限场景；ZeRO-3内存节省最大，但需要高带宽互联；实际部署中常采用混合策略，如ZeRO-2配合梯度累积。ZeRO Offload技术还能将部分状态卸载到CPU内存，进一步扩展训练规模。

## Deepspeed ZeRO 1/2/3 通信流程 <Badge text="重要" type="danger" />

**ZeRO-1：优化器状态划分**
ZeRO-1的通信模式最为简单直接。在前向和反向传播过程中，每个进程都持有完整的模型参数和梯度，因此无需任何额外通信，计算完全独立。通信开销集中在优化器步骤：反向传播后，各进程首先对完整梯度进行All-Reduce求平均（与传统数据并行相同），接着每个进程仅更新自己负责的优化器状态分区，最后通过All-Gather广播更新后的参数分区，使所有进程获得完整参数。总通信量约为模型参数量的3倍，与传统数据并行完全一致，但通过优化器状态划分实现了约4倍的内存节省。

**ZeRO-2：增加梯度划分**
ZeRO-2引入了更复杂的通信模式以换取更大的内存节省。前向传播需要参数收集通信，因为每个进程只存储自己负责的梯度分区对应的参数分区，计算前必须通过All-Gather从所有进程收集完整参数，每次前向传播通信量为参数量Ψ。反向传播分为两个阶段：各进程先独立计算完整梯度，然后通过Reduce-Scatter操作进行梯度规约，各进程只保留自己负责的梯度分区，通信量为Ψ（比传统减少一半）。优化器步骤中，各进程仅更新自己分区的参数，无需额外同步通信。总通信量为2Ψ，相比传统数据并行的3Ψ减少约33%，同时内存节省扩大到约8倍。

**ZeRO-3：完整参数划分**
ZeRO-3采用最激进的划分策略，通信模式也最为复杂。前向传播需要实时参数获取，每个层计算时动态通过All-Gather获取所需参数，采用持久化进程组和计算-通信重叠等技术优化。反向传播同样需要All-Gather获取完整参数，然后通过Reduce-Scatter进行梯度规约和分区。参数更新后采用惰性广播机制，在下次需要时自动获取新参数。

ZeRO-3的通信特点是细粒度和高频次，每层都可能触发通信，但单次通信量较小。实际通信开销通常高于理论值2Ψ，因为存在元数据通信和同步开销。在高速互联环境下，ZeRO-3效率接近ZeRO-2；在慢速网络中性能下降明显。其最大优势是内存节省与GPU数量成比例，适合训练远超单卡内存容量的超大模型。

## Megatron-LM <Badge text="掌握" type="tip" />

Megatron-LM的核心贡献是多维度并行策略的协同设计，将张量并行、流水线并行和数据并行有机结合。张量并行专注于单层内部的切分，将矩阵运算分布到多个设备，通过All-Reduce通信保持数学等价性；流水线并行实施层间切分，形成模型计算流水线，需要仔细调度微批次以最小化设备闲置时间；数据并行则进行批次维度切分，各设备处理不同数据分片。

该框架特别对Transformer架构进行了深度优化：注意力计算分割将多头注意力分散到不同设备；层归一化位置调整减少通信需求；梯度累积策略平衡内存与吞吐量。Megatron的通信优化尤为突出，通过计算与通信重叠、梯度压缩等技术，在数千GPU集群上仍能保持较高计算效率。

实际部署经验表明，张量并行最适合小规模紧密连接设备（如NVLink连接的8卡节点），流水线并行适合设备间带宽有限场景，而3D并行组合可扩展至数万GPU训练万亿参数模型。调优时需特别注意气泡时间、通信开销和负载平衡的权衡。

## LLaMA-Factory <Badge text="了解" type="info" />

LLaMA-Factory定位为端到端的LLaMA模型工程平台，将模型微调、评估和部署流程工具化、标准化。其主要价值在于降低大模型定制化门槛，通过模块化设计支持从数据预处理到模型服务的完整生命周期。

框架设计上强调灵活性与效率的平衡：一方面支持多种微调范式，从全参数微调到参数高效方法如LoRA、QLoRA，再到强化学习对齐如PPO、DPO；另一方面提供资源感知优化，包括梯度检查点、混合精度训练、CPU卸载等技术，使大模型能在消费级硬件上微调。

实际应用中的关键特性包括：统一的数据接口支持多种格式转换；可配置的训练流水线支持多阶段微调；集成的评估体系涵盖从基础能力到安全对齐的多维度测试；便捷的部署工具支持量化、剪枝等压缩技术。