# 强化学习

## 基础概念 <Badge text="重要" type="danger" />

强化学习（RL）是智能体通过与环境交互，以试错方式学习策略的机器学习范式，核心是通过奖励信号引导行为优化，实现从状态到动作的映射，目标是最大化长期累积奖励。核心要素包含：智能体（Agent）、环境（Environment）、状态（State）、动作（Action）、奖励（Reward）、策略（Policy）、价值函数（Value Function）、马尔可夫决策过程（MDP，RL 问题的数学建模框架）。



## 强化学习方法分类 <Badge text="掌握" type="tip" />

按模型知识：

- 模型基于（Model-based）：学习环境动态模型

- 模型无关（Model-free）：直接学习策略/价值函数

按学习方式：

- 价值基于（Value-based）：学习价值函数（如Q-learning、DQN）

- 策略基于（Policy-based）：直接学习策略函数（如REINFORCE）

- 演员-评论家（Actor-Critic）：结合两者（如A2C、PPO）

按策略类型：

- 同策略（On-policy）：使用当前策略收集数据（如PPO）

- 异策略（Off-policy）：使用不同策略收集数据（如DQN、DDPG）

按数据来源分：

- 在线强化学习：智能体与环境进行实时交互、实时获得反馈并学习更新策略。

- 离线强化学习：智能体与环境无实时交互，通过事先收集的离线数据集学习。

## PPO原理及其优缺点 <Badge text="重要" type="danger" />

[PPO原理请参考这个链接🔗](https://zhuanlan.zhihu.com/p/677607581)

PPO（近端策略优化）的核心思想是通过限制策略更新的幅度来确保训练稳定性。它采用裁剪机制约束新旧策略的差异，避免因单次更新过大而导致策略性能崩溃。具体实现中，PPO计算新旧策略的概率比，将其限制在[1-ε, 1+ε]范围内，然后基于优势函数优化目标。

调试时需要综合观察多个指标：回报曲线、KL散度、裁剪频率和价值损失。如果裁剪频率超过15%，应考虑增大ε；若低于5%可减小ε。价值损失应与策略损失保持同量级，如果过大则需要降低价值网络学习率。回报曲线的剧烈波动往往表明学习率过大或批次大小不足。

PPO的主要优点是实现相对简单、训练稳定、通用性强，适用于各类任务。缺点在于超参数仍需仔细调整、理论保证较弱、探索能力有限，且作为同策略算法在样本效率上不如异策略方法。

## DPO的原理及其优缺点 <Badge text="重要" type="danger" />

[DPO、GRPO原理可以参考这个链接🔗](https://zhuanlan.zhihu.com/p/1984387073625593089)
DPO（直接偏好优化）是一种绕过显式奖励建模的强化学习方法。它基于Bradley-Terry模型，直接利用人类偏好数据优化策略，将强化学习问题转化为偏好分类问题。DPO的核心洞察是，对于最优策略，其偏好概率与奖励差异之间存在解析关系，因此可以直接从偏好数据中学习策略，而无需训练单独的奖励模型。

DPO的优点主要体现在简化训练流程、提高计算效率和增强稳定性方面。它消除了奖励模型的训练环节，避免了奖励函数设计困难、奖励劫持和过度优化等问题。同时，DPO直接优化策略偏好，训练过程更加稳定可控。

然而DPO也存在明显局限。它对偏好数据的质量和数量要求很高，需要覆盖多样化的场景和比较对；难以融入额外的奖励信号或约束条件；在新领域的泛化能力可能受限；且缺乏显式奖励函数，不利于后续的分析和调试。尽管如此，在语言模型对齐等任务中，DPO因其简单高效而成为重要方法。  

## GRPO和PPO有什么区别 <Badge text="重要" type="danger" />

GRPO（分组相对策略优化）在PPO基础上引入了分组比较机制。核心改进在于将样本分组，在组内计算相对奖励进行标准化，而非使用传统的优势函数估计。这种设计能更好地处理奖励稀疏和方差大的问题，提升训练稳定性。

具体区别体现在几个方面：奖励处理上，GRPO采用组内相对比较，减少了全局奖励方差；更新机制上，GRPO以组为单位进行策略优化，支持组内样本的重复利用；并行效率上，GRPO更适合大规模并行环境，在多智能体任务中表现出色。GRPO通过分组比较降低了估计方差，在奖励稀疏的任务中通常表现更稳定，但实现相对复杂，对分组策略敏感。

## SFT是必要的吗 <Badge text="掌握" type="tip" />
> 关键词：冷启动、涌现

监督微调（SFT）的必要性需结合具体场景评估。在强化学习尤其是语言模型对齐中，SFT通常作为重要的预处理阶段，为后续的强化学习提供合理的初始策略。这对于冷启动至关重要，能够避免从完全随机策略开始探索的低效性，并确保基本的安全性和能力水平。SFT利用现有标注数据快速建立基础能力，为后续的强化学习提供必要的技能基础。

然而，SFT并非绝对必需。当环境允许低成本试错探索、奖励信号设计明确且丰富、或者任务需要完全自主的探索发现时，可以直接从强化学习开始。在计算资源受限的情况下，跳过SFT直接进行强化学习有时会更高效。(Deepseek-V3跳过了SFT，直接上RL, 出现了涌现现象)

实际应用中需要权衡多方面因素：任务的安全要求、可用标注数据的质量和数量、计算资源限制以及最终性能目标等。在大型语言模型的训练中，SFT通常被视为关键环节，它为后续的偏好对齐提供了必要的初始能力和安全性基础，但具体实施时应根据任务特性和资源约束做出合理决策。